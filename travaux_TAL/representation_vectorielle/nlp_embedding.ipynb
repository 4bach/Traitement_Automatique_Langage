{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 : Neural Embeddings, Text Classification, Text Generation\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **bag of word** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## Dataset\n",
    "https://github.com/cedias/practicalNLP/tree/master/dataset\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (only here for reference)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Loading data (same as in nlp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"/tmp/dataset/NLP/practicalNLP-master/dataset/json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train (or load) a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 16:11:04,303 : INFO : collecting all words and their counts\n",
      "2019-04-11 16:11:04,304 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-11 16:11:04,676 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n",
      "2019-04-11 16:11:05,061 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2019-04-11 16:11:05,256 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2019-04-11 16:11:05,256 : INFO : Loading a fresh vocabulary\n",
      "2019-04-11 16:11:05,558 : INFO : effective_min_count=5 retains 49345 unique words (17% of original 280617, drops 231272)\n",
      "2019-04-11 16:11:05,558 : INFO : effective_min_count=5 leaves 5517507 word corpus (94% of original 5844680, drops 327173)\n",
      "2019-04-11 16:11:05,662 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2019-04-11 16:11:05,670 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2019-04-11 16:11:05,670 : INFO : downsampling leaves estimated 4268608 word corpus (77.4% of prior 5517507)\n",
      "2019-04-11 16:11:05,824 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2019-04-11 16:11:05,824 : INFO : resetting layer weights\n",
      "2019-04-11 16:11:06,185 : INFO : training model with 3 workers on 49345 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-11 16:11:07,196 : INFO : EPOCH 1 - PROGRESS: at 10.17% examples, 439786 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:08,230 : INFO : EPOCH 1 - PROGRESS: at 21.09% examples, 444404 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:09,233 : INFO : EPOCH 1 - PROGRESS: at 31.57% examples, 446215 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:10,234 : INFO : EPOCH 1 - PROGRESS: at 42.19% examples, 447253 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:11,244 : INFO : EPOCH 1 - PROGRESS: at 52.76% examples, 448850 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:12,253 : INFO : EPOCH 1 - PROGRESS: at 63.41% examples, 449646 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:13,263 : INFO : EPOCH 1 - PROGRESS: at 74.48% examples, 449717 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:14,274 : INFO : EPOCH 1 - PROGRESS: at 85.17% examples, 449914 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:15,283 : INFO : EPOCH 1 - PROGRESS: at 95.75% examples, 450149 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:15,644 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-11 16:11:15,656 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-11 16:11:15,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-11 16:11:15,668 : INFO : EPOCH - 1 : training on 5844680 raw words (4269274 effective words) took 9.5s, 450443 effective words/s\n",
      "2019-04-11 16:11:16,679 : INFO : EPOCH 2 - PROGRESS: at 10.17% examples, 439532 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:17,683 : INFO : EPOCH 2 - PROGRESS: at 20.76% examples, 443538 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:18,711 : INFO : EPOCH 2 - PROGRESS: at 30.79% examples, 435000 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:19,718 : INFO : EPOCH 2 - PROGRESS: at 41.40% examples, 438127 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:20,729 : INFO : EPOCH 2 - PROGRESS: at 51.64% examples, 439956 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:21,737 : INFO : EPOCH 2 - PROGRESS: at 62.42% examples, 442261 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:22,744 : INFO : EPOCH 2 - PROGRESS: at 73.44% examples, 443567 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:23,752 : INFO : EPOCH 2 - PROGRESS: at 84.19% examples, 444716 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:24,760 : INFO : EPOCH 2 - PROGRESS: at 94.82% examples, 445451 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:25,220 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-11 16:11:25,231 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-11 16:11:25,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-11 16:11:25,248 : INFO : EPOCH - 2 : training on 5844680 raw words (4268081 effective words) took 9.6s, 445670 effective words/s\n",
      "2019-04-11 16:11:26,255 : INFO : EPOCH 3 - PROGRESS: at 10.17% examples, 440334 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:27,256 : INFO : EPOCH 3 - PROGRESS: at 20.76% examples, 444860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:28,260 : INFO : EPOCH 3 - PROGRESS: at 31.30% examples, 446525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:29,277 : INFO : EPOCH 3 - PROGRESS: at 42.04% examples, 447354 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:30,280 : INFO : EPOCH 3 - PROGRESS: at 52.34% examples, 448182 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:31,305 : INFO : EPOCH 3 - PROGRESS: at 63.07% examples, 447965 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:32,316 : INFO : EPOCH 3 - PROGRESS: at 74.14% examples, 448156 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:33,328 : INFO : EPOCH 3 - PROGRESS: at 84.84% examples, 448525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:34,337 : INFO : EPOCH 3 - PROGRESS: at 95.45% examples, 448860 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:34,726 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-11 16:11:34,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-11 16:11:34,739 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-11 16:11:34,740 : INFO : EPOCH - 3 : training on 5844680 raw words (4269044 effective words) took 9.5s, 449847 effective words/s\n",
      "2019-04-11 16:11:35,763 : INFO : EPOCH 4 - PROGRESS: at 10.35% examples, 441907 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:36,769 : INFO : EPOCH 4 - PROGRESS: at 20.92% examples, 444411 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:37,783 : INFO : EPOCH 4 - PROGRESS: at 31.75% examples, 448935 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:38,794 : INFO : EPOCH 4 - PROGRESS: at 41.88% examples, 443079 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:39,797 : INFO : EPOCH 4 - PROGRESS: at 51.85% examples, 441778 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:40,798 : INFO : EPOCH 4 - PROGRESS: at 62.42% examples, 443098 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:41,805 : INFO : EPOCH 4 - PROGRESS: at 73.44% examples, 444239 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:42,816 : INFO : EPOCH 4 - PROGRESS: at 84.19% examples, 445115 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:43,823 : INFO : EPOCH 4 - PROGRESS: at 94.82% examples, 445897 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:44,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-11 16:11:44,299 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-11 16:11:44,301 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-11 16:11:44,302 : INFO : EPOCH - 4 : training on 5844680 raw words (4266752 effective words) took 9.6s, 446514 effective words/s\n",
      "2019-04-11 16:11:45,316 : INFO : EPOCH 5 - PROGRESS: at 10.17% examples, 437938 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:46,333 : INFO : EPOCH 5 - PROGRESS: at 20.92% examples, 443495 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:47,344 : INFO : EPOCH 5 - PROGRESS: at 31.46% examples, 444538 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-11 16:11:48,359 : INFO : EPOCH 5 - PROGRESS: at 42.19% examples, 446120 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:49,367 : INFO : EPOCH 5 - PROGRESS: at 52.53% examples, 446607 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:50,387 : INFO : EPOCH 5 - PROGRESS: at 63.07% examples, 445816 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:51,403 : INFO : EPOCH 5 - PROGRESS: at 74.14% examples, 445998 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:52,422 : INFO : EPOCH 5 - PROGRESS: at 84.84% examples, 446284 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:53,440 : INFO : EPOCH 5 - PROGRESS: at 95.45% examples, 446325 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-11 16:11:53,831 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 16:11:53,835 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-11 16:11:53,848 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-11 16:11:53,848 : INFO : EPOCH - 5 : training on 5844680 raw words (4267792 effective words) took 9.5s, 447204 effective words/s\n",
      "2019-04-11 16:11:53,849 : INFO : training on a 29223400 raw words (21340943 effective words) took 47.7s, 447740 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's for later\n",
    "\n",
    "#from gensim.test.utils import datapath\n",
    "#w2v = KeyedVectors.load_word2vec_format(datapath('downloaded_vectors_path'), binary=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7710139\n",
      "great and bad: 0.47024545\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 16:12:12,888 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('film', 0.9415352940559387),\n",
       " ('\"film\"', 0.8324918746948242),\n",
       " ('\"movie\"', 0.7933178544044495),\n",
       " ('flick', 0.7839542627334595),\n",
       " ('movie...', 0.775497555732727)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', 0.7751139402389526),\n",
       " ('dreadful', 0.6656144261360168),\n",
       " ('crappy', 0.6569118499755859)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 16:13:07,076 : INFO : Evaluating word analogies for top 300000 words in the model on /tmp/dataset/NLP/practicalNLP-master/dataset/questions-words.txt\n",
      "2019-04-11 16:13:07,077 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-11 16:13:07,345 : INFO : capital-common-countries: 1.3% (2/156)\n",
      "2019-04-11 16:13:07,546 : INFO : capital-world: 0.9% (1/111)\n",
      "2019-04-11 16:13:07,605 : INFO : currency: 0.0% (0/18)\n",
      "2019-04-11 16:13:08,049 : INFO : city-in-state: 0.0% (0/301)\n",
      "2019-04-11 16:13:08,683 : INFO : family: 30.7% (129/420)\n",
      "2019-04-11 16:13:10,193 : INFO : gram1-adjective-to-adverb: 2.2% (19/870)\n",
      "2019-04-11 16:13:11,050 : INFO : gram2-opposite: 3.3% (18/552)\n",
      "2019-04-11 16:13:12,789 : INFO : gram3-comparative: 18.2% (217/1190)\n",
      "2019-04-11 16:13:13,906 : INFO : gram4-superlative: 11.6% (88/756)\n",
      "2019-04-11 16:13:15,123 : INFO : gram5-present-participle: 17.1% (139/812)\n",
      "2019-04-11 16:13:16,514 : INFO : gram6-nationality-adjective: 1.3% (13/967)\n",
      "2019-04-11 16:13:18,371 : INFO : gram7-past-tense: 19.3% (243/1260)\n",
      "2019-04-11 16:13:19,559 : INFO : gram8-plural: 7.8% (63/812)\n",
      "2019-04-11 16:13:20,509 : INFO : gram9-plural-verbs: 29.5% (192/650)\n",
      "2019-04-11 16:13:20,530 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2019-04-11 16:13:20,534 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-04-11 16:13:20,536 : INFO : Total accuracy: 12.7% (1124/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"/tmp/dataset/NLP/practicalNLP-master/dataset/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.7886042e+01 -7.8027743e-01 -4.1003257e-01 -2.1373482e+01\n",
      "  6.1560082e+00 -2.0894817e+01  1.0389733e+00 -2.5070616e+01\n",
      " -2.5706573e+01 -1.2424294e+01  1.2535731e+01  6.1059103e+00\n",
      " -1.1380868e+01  6.4855542e+00 -3.4474487e+01 -3.5920491e+00\n",
      " -9.4662075e+00 -1.1681905e+01  1.8491266e+01  4.4480200e+00\n",
      " -1.2375173e+01  9.2218637e+00  2.0559921e+01  5.7406468e+00\n",
      " -2.2840813e+01  3.9311182e+00 -6.7368813e+00 -1.2828704e+01\n",
      "  9.3687277e+00  7.4983163e+00 -5.2785659e+00 -2.7230198e+01\n",
      "  6.8447328e+00 -1.6500832e+01 -9.0638828e+00  1.3208472e+01\n",
      "  9.3684082e+00 -3.8425870e+00  2.9213195e+00 -5.0729518e+00\n",
      " -9.9571705e+00  7.0591149e+00  1.7749706e+01  1.2138098e+01\n",
      " -9.8159510e-01 -2.8610676e+01 -2.9535234e+00 -3.3231330e+00\n",
      "  1.3111875e+01 -4.5923052e+00  2.9014984e+01 -2.5842575e+01\n",
      " -7.0809450e+00  2.0500164e+01  3.4265206e+00 -4.5066137e+00\n",
      " -3.2824352e+00  2.3740061e+01 -2.0158443e+01 -5.5464048e+00\n",
      " -1.4682545e+01  2.7118500e+01 -3.7663555e-01 -2.1104574e+01\n",
      "  2.8361285e+00  1.8155472e+01  1.8482280e+00  1.7619892e+01\n",
      " -1.6265593e+01  2.5746597e+01  7.6846490e+00  3.0658712e+00\n",
      " -1.5498075e+01  9.8126163e+00 -1.9656136e+01 -2.7409386e+01\n",
      " -8.2302322e+00 -7.0711656e+00  2.7607395e+01 -2.5101578e+01\n",
      "  1.0997042e+01 -1.7345400e+01  3.7695322e+00 -2.7834063e+01\n",
      " -3.8841257e+00 -1.8909054e+01 -9.6035004e+00 -5.0805101e+00\n",
      " -3.4257553e+01  2.4221794e+01 -8.3205116e-01  2.2708349e+01\n",
      "  4.7538319e+00 -5.8934650e+00  1.4078876e+01  2.4695076e+01\n",
      "  9.6003284e+00 -1.2268879e+01  5.7903733e-03  6.2762499e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "vocab = w2v.wv.vocab\n",
    "def vectorize(text,mean=False):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "    vec = 0 # pfffff\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "            vec += w2v.wv[word]\n",
    "         \n",
    "        \n",
    "        \n",
    "    return np.array(vec)\n",
    "    \n",
    "\n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text,pol in train]\n",
    "X_test = [vectorize(text) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(X,open( \"/tmp/dataset/NLP/practicalNLP-master/dataset/objects/X.p\", \"wb\" ))\n",
    "pickle.dump(X_test,open('/tmp/dataset/NLP/practicalNLP-master/dataset/objects/X_test.p',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81868"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "clf = LogisticRegression().fit(X, classes)\n",
    "Ypred = clf.predict(X_test)\n",
    "accuracy_score(Ypred,true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a pas une grosse différence entre les deux modèles. Cbow est plus rapide que skip gram mais les résultats sont globalement les mêmes. Cbow prend le contexte sachant les mots et SG l'inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('/tmp/dataset/NLP/practicalNLP-master/dataset/input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[18, 29, 94, 17, 24, 23, 24, 30, 27, 10],\n",
      "        [17, 14, 94, 29, 18, 13, 14, 94, 29, 24],\n",
      "        [96, 96, 52, 56, 40, 40, 49, 94, 48, 36],\n",
      "        [10, 18, 27, 94, 11, 27, 14, 10, 29, 17]]), tensor([[29, 94, 17, 24, 23, 24, 30, 27, 10, 11],\n",
      "        [14, 94, 29, 18, 13, 14, 94, 29, 24, 94],\n",
      "        [96, 52, 56, 40, 40, 49, 94, 48, 36, 53],\n",
      "        [18, 27, 94, 11, 27, 14, 10, 29, 17, 14]]))\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = nn.Embedding(n_char,hidden_size)\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = nn.RNN(hidden_size,hidden_size)\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 2s (100 0%) 2.5427]\n",
      "Whapald panle amare oforeatooullen t wisoo ch seefingee, u an theng lld he aro id ng ate wiThe mbedmen \n",
      "\n",
      "[0m 4s (200 1%) 2.5285]\n",
      "Wh o tenou tstofrthtoundef tespy anthe no, tr ro my wadNGo t tiI bouly t avest malerr is theyodeak de, \n",
      "\n",
      "[0m 6s (300 1%) 2.4548]\n",
      "Whathag pp lind h my w buroures wa n st or st\n",
      "Fot mpese! lo ty at w.\n",
      "O tor ol I ye t istan y he ye I s \n",
      "\n",
      "[0m 8s (400 2%) 2.5103]\n",
      "What thand ce orsthes.\n",
      "Anen d nge llate t n me be and nd,\n",
      "Wind;\n",
      "\n",
      "\n",
      "Fowhowicithave t he mur,\n",
      "SO:\n",
      "Thou ue \n",
      "\n",
      "[0m 10s (500 2%) 2.4441]\n",
      "Wheth.\n",
      "\n",
      "ONTh hoD te FO:\n",
      "Ta th y the sthel ho athis st t hebe the weloul y,\n",
      "Whithoucranoun s ctherofin  \n",
      "\n",
      "[0m 12s (600 3%) 2.5284]\n",
      "Wheakealld e thoreferofes ind cit uerer we h ativevea tharears peat Theshou he s, fouruedir sin tllan  \n",
      "\n",
      "[0m 14s (700 3%) 2.4218]\n",
      "Who f me! t thediglland thor fe, tllane tire st--gr:\n",
      "ING:\n",
      "\n",
      "I withowoned!\n",
      "Tom, and t thasut anth thatha \n",
      "\n",
      "[0m 17s (800 4%) 2.4251]\n",
      "Wher powaverthe ts\n",
      "Thou fomengs ourer my s fit akneprr tr myerin ik aichellis sell hane pplor the.\n",
      "Th  \n",
      "\n",
      "[0m 19s (900 4%) 2.4782]\n",
      "Whet pre sth ala o d we are o,\n",
      "ATha t liloceshe y od thitte wie w bakn t ly w hen Joncheeanthetoais o  \n",
      "\n",
      "[0m 21s (1000 5%) 2.4532]\n",
      "Where t, beas tore irth talear I t bend wakndvelo thansthe he se w.\n",
      "Durourmowhet ss hayeat iton oues h \n",
      "\n",
      "[0m 23s (1100 5%) 2.4933]\n",
      "Why, gh man sin l grrthan terbus\n",
      "ar ily s towear, thil ininigo peay anathid ll s hore IUS:\n",
      "SThat wan h \n",
      "\n",
      "[0m 25s (1200 6%) 2.5043]\n",
      "Wher the t cefre her ak sere ntea at grus t t pr p, cerioude he she'sererd; ines, t the, be ct ma w d  \n",
      "\n",
      "[0m 27s (1300 6%) 2.4255]\n",
      "Who d-bly st me faly sil I gert athin ke tu y tir melows ry ld warsthint athal ator t s st s ind shing \n",
      "\n",
      "[0m 29s (1400 7%) 2.4355]\n",
      "Whth the hour as Frindse r theraze d hefeswe CAndsou owe d g aze ashe forthe atar he owinour d to's th \n",
      "\n",
      "[0m 31s (1500 7%) 2.4621]\n",
      "Whand ws loul thand s send ofupr th thespe me n spren brd me ckensothenowis aveathavicour m, wethe, an \n",
      "\n",
      "[0m 33s (1600 8%) 2.4676]\n",
      "Whathe bre ain s\n",
      "\n",
      "IO:\n",
      "Pore gen.\n",
      "\n",
      "BUMy whe\n",
      "HAnayows maneedswad sthefichen,\n",
      "TI is\n",
      "RWishitoth d as ay s t \n",
      "\n",
      "[0m 36s (1700 8%) 2.4484]\n",
      "Whaulyof wons my bed nd land hang s.\n",
      "Anthereng t thald\n",
      "HAnof d, pry shons ay avine t m the, the t.\n",
      "I d \n",
      "\n",
      "[0m 38s (1800 9%) 2.4469]\n",
      "Whe b f arre andenour sthaisomathe hine,\n",
      "K:\n",
      "\n",
      "O:\n",
      "\n",
      "Yoorer tho manerow, hinit nd qutthell me w'sint bllor \n",
      "\n",
      "[0m 40s (1900 9%) 2.4435]\n",
      "Wh acofoul ten l nowen apomef p, anong-ou h y orpe ther an t py ofat tid ll thasave; d, isuls boren\n",
      "AR \n",
      "\n",
      "[0m 42s (2000 10%) 2.4984]\n",
      "Wh bld the wimane,\n",
      "\n",
      "To rr, uiverce id gar the ie\n",
      "St wispl winene HES: estooriootheasour's t athe grtif \n",
      "\n",
      "[0m 44s (2100 10%) 2.4770]\n",
      "Whinor whitel ato he sthen eas\n",
      "\n",
      "The me watee bowntou an lllor ha athave HI o es\n",
      "Fotinearsave; fould he \n",
      "\n",
      "[0m 47s (2200 11%) 2.4333]\n",
      "Wh susmy wavake, t harndomilit cis,\n",
      "S:\n",
      "Shesis I itear pas; de t HUSTO:\n",
      "The:\n",
      "\n",
      "TRI INI wind indrior?\n",
      "\n",
      "MA \n",
      "\n",
      "[0m 49s (2300 11%) 2.4228]\n",
      "Wh mar y t'lalfu ar my ofiss m bere find d be blloun,\n",
      "When the ls bethan ce t y en wisth w bl is'l owe \n",
      "\n",
      "[0m 51s (2400 12%) 2.4403]\n",
      "Whur.\n",
      "fadite p s; the thand l I t thas d\n",
      "ARI to rer\n",
      "\n",
      "\n",
      "Th y, Whould nd w,\n",
      "Whin san as arom me fo mbothi \n",
      "\n",
      "[0m 53s (2500 12%) 2.4285]\n",
      "Who cars, y k by, anoncas u.\n",
      "Yow ane this the INGBORIUCarourerin win wo o S:\n",
      "\n",
      "Fisores the lll s pe brd \n",
      "\n",
      "[0m 55s (2600 13%) 2.5019]\n",
      "Wh hadoutind s t or me athiche ngrsts oul my o se od ferend he Thineng t f l therce owhy es t ausipoth \n",
      "\n",
      "[0m 57s (2700 13%) 2.4829]\n",
      "Wha p wal myofar isponen:\n",
      "Ho t woferano l tis no, he,\n",
      "BO:\n",
      "I iof s goth im,\n",
      "\n",
      "\n",
      "\n",
      "Ass ipeid rir, siengre R \n",
      "\n",
      "[0m 59s (2800 14%) 2.5192]\n",
      "Whes t beer bo.\n",
      "\n",
      "\n",
      "Tords,\n",
      "\n",
      "\n",
      "Ofes s we th st wondores w, s fo wind t.\n",
      "O folino bea u heyol!\n",
      "NTharaisthes \n",
      "\n",
      "[1m 1s (2900 14%) 2.4836]\n",
      "Whorillle is, Bewathiande ng here thin s mang tar thereag ame the thin thar I ROLLARI bove owerellon s \n",
      "\n",
      "[1m 3s (3000 15%) 2.3895]\n",
      "Whe the:\n",
      "TI batha whens me ies wie wad tholist. ghane hin ate m athais wee thithesareay mankithenost b \n",
      "\n",
      "[1m 6s (3100 15%) 2.4939]\n",
      "Whate owhathe:\n",
      "O:\n",
      "Thisakerow d oulomar.\n",
      "s thugathatulyord, her ode gr grioars\n",
      "There omy burd Wh sa gar \n",
      "\n",
      "[1m 8s (3200 16%) 2.4517]\n",
      "Whalis by mo priviss is d he t sieryen ite nge, fooowifithe y he be ll sond t w, mat bourou, athe thed \n",
      "\n",
      "[1m 10s (3300 16%) 2.4572]\n",
      "Whed witsteshes myothtir yo hy anol.\n",
      "\n",
      "Anoron he h, ak'sithige\n",
      "\n",
      "PE:\n",
      "Fir, or care wirts BUCHI s thathe s \n",
      "\n",
      "[1m 12s (3400 17%) 2.4893]\n",
      "Whail ter andin thengrilld owindoe hot, t an hor-benonchell the bre:\n",
      "THath figer.\n",
      "Fowowhit athes tre o \n",
      "\n",
      "[1m 14s (3500 17%) 2.3902]\n",
      "Whe, ist dindine mal;\n",
      "I thofe, t IOUS ne than thes, han billir ball wh ilou nd key pe fieancashy e ll  \n",
      "\n",
      "[1m 16s (3600 18%) 2.5274]\n",
      "Whay chake.\n",
      "Angets aithal;\n",
      "And ot met aitheain thay thathougru the wathom:\n",
      "TOFounthe athe ongs. orerta \n",
      "\n",
      "[1m 18s (3700 18%) 2.4664]\n",
      "Whaiclorr I ep,\n",
      "I I thanour ad sheee wieer f ty t f me hot I ino thaniret viss tis buste t men O:\n",
      "S: m \n",
      "\n",
      "[1m 20s (3800 19%) 2.5187]\n",
      "Whe tousllo thun he re se.\n",
      "Or couth l s she.\n",
      "\n",
      "PELENRurerut l hy foulle groucofrleare t pou as m t,\n",
      "D:\n",
      " \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-0e2d8e088fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#train on one chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-9691abf743b2>\u001b[0m in \u001b[0;36mrandom_training_set\u001b[0;34m(chunk_len, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-9691abf743b2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-9691abf743b2>\u001b[0m in \u001b[0;36mchar_tensor\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_characters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 20000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba0ebc6208>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tis s wnd oine wind t'd me serkerpot? dis thoud?\n",
      "Shexist t it head tnomoookeruronds kirerinorerd mmusthan ck neopauis, ben h s, thaghig ss the\n",
      "PO:\n",
      "St,\n",
      "Wat nyoutowe o perif-deimatoost r f t sthevea\n",
      "----\n",
      "Theivin hande he be meameded irof d toulled for ter th\n",
      "\n",
      "HARI ind sthan s, t t polead s ay ver, ttik hereis, t IO:\n",
      "\n",
      "ULAnd the meyod bes s ms keasthendy keng or ngres?\n",
      "S:\n",
      "\n",
      "\n",
      "LIn hek arir se pl meartt ho tr\n",
      "----\n",
      "The there ar h th te,\n",
      "\n",
      "Son my t re tou y cal m te tow al ber m\n",
      "Whand an the t ghend s y thald hof t the the hand the the thofour the e the he thar fon that ENo t rmay thind oure thime be t we mave athed\n",
      "----\n",
      "The me our t ar n the be the the the the s thand t bere t me t mere me me t the y bend al t t the t be the that thar t the s the t thes thand the thers s pand the t t the the mer mathe the thar t the t \n",
      "----\n",
      "The the t the the the the the t the the the t the the the t the the the the the the the the the the the the the the the the the the the the the the the the the the me the the the the the the the t the t\n"
     ]
    }
   ],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ End of practical\n",
    "\n",
    "#### Legacy loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-eee45b9193a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#contains (text,pol) couples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from os.path import split as pathsplit\n",
    "\n",
    "dir_train = \"dataset/aclImdb/train/\"\n",
    "dir_test = \"dataset/aclImdb/test/\"\n",
    "\n",
    "train_files = glob.glob(dir_train+'pos/*.txt') + glob.glob(dir_train+'neg/*.txt')\n",
    "test_files = glob.glob(dir_test+'pos/*.txt') + glob.glob(dir_test+'neg/*.txt')\n",
    "\n",
    "\n",
    "def get_polarity(f):\n",
    "    \"\"\"\n",
    "    Extracts polarity from filename:\n",
    "    0 is negative (< 5)\n",
    "    1 is positive (> 5)\n",
    "    \"\"\"\n",
    "    _,name = pathsplit(f)\n",
    "    if int(name.split('_')[1].split('.')[0]) < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def open_one(f):\n",
    "    \n",
    "    polarity = get_polarity(f)\n",
    "    \n",
    "    with open(f,\"r\") as review:\n",
    "        text = \" \".join(review.readlines()).strip()\n",
    "    \n",
    "    return (text,polarity)\n",
    "\n",
    "print(open_one(train_files[0]))\n",
    "\n",
    "train = [open_one(x) for x in train_files] #contains (text,pol) couples\n",
    "test = [open_one(x) for x in test_files]   #contains (text,pol) couples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
